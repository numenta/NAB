import os
import math
import simplejson as json

from nupic.algorithms import anomaly_likelihood
from nupic.frameworks.opf.modelfactory import ModelFactory

from nab.detectors.base import AnomalyDetector

class NumentaDetector(AnomalyDetector):

  def __init__(self, *args, **kwargs):

    # Init the super class
    super(NumentaDetector, self).__init__(*args, **kwargs)



  def getOutputPrefix(self):
    """
    Returns the string to prepend to results files generated by this class
    """
    return "numenta"

  def getAdditionalHeaders(self):
    """
    Returns a list of strings.
    """
    return ["_raw_score"]

  def getThreshold(self):
    """
    Returns a known good threshold for the dataset. Discovered by using the
    optimize_threshold.py script.
    """

    return 0.99996

  def handleRecord(self, inputData):
    """
    Returns a list [anomalyScore, rawScore].

    Internally to NuPIC "anomalyScore" corresponds to "likelihood_score"
    and "rawScore" corresponds to "anomaly_score". Sorry about that.
    """
    # print type(self.model)
    # print "inputData: %s" % str(inputData)
    # Send it to Numenta detector and get back the results
    result = self.model.run(inputData)

    # print "result: %s" % str(result)
    # Retrieve the anomaly score and write it to a file
    rawScore = result.inferences["anomalyScore"]

    # print "rawScore: %s" %(rawScore)
    # Compute the Anomaly Likelihood
    anomalyScore = self.anomalyLikelihood.likelihood(inputData["value"],
                                                     rawScore,
                                                     inputData["timestamp"])
    # print "anomalyScore: %s" % str(anomalyScore)

    return [anomalyScore, rawScore]

  def configureDetector(self, probationaryPeriodData):
    calcRange = abs(self.inputMax - self.inputMin)
    calcPad = calcRange * .2

    self.inputMin = self.inputMin - calcPad
    self.inputMax = self.inputMax + calcPad
        # Load the model params JSON
    probationaryPeriod = probationaryPeriodData.shape[0]

    paramsPath = os.path.join(os.path.split(__file__)[0],
                "modelParams",
                "model_params.json")
    with open(paramsPath) as fp:
      modelParams = json.load(fp)

    self.sensorParams = modelParams["modelParams"]["sensorParams"]\
                                   ["encoders"]["value"]

    # RDSE - resolution calculation
    resolution = max(0.001,
                     (self.inputMax - self.inputMin) / \
                     self.sensorParams.pop("numBuckets")
                    )
    self.sensorParams["resolution"] = resolution

    self.model = ModelFactory.create(modelParams)

    self.model.enableInference({"predictedField": "value"})

    # The anomaly likelihood object
    numentaLearningPeriod = math.floor(probationaryPeriod / 2.0)
    self.anomalyLikelihood = AnomalyLikelihood(probationaryPeriod,
                                               numentaLearningPeriod)

#############################################################################

class AnomalyLikelihood(object):
  """
  Helper class for running anomaly likelihood computation.
  """

  def __init__(self, probationaryPeriod = 600, numentaLearningPeriod = 300):
    """
    probationaryPeriod - no anomaly scores are reported for this many
    iterations.  This should be numentaLearningPeriod + some number of records
    for getting a decent likelihood estimation.

    numentaLearningPeriod - the number of iterations required for the Numenta
    detector to learn some of the patterns in the dataset.
    """
    self._iteration          = 0
    self._historicalScores   = []
    self._distribution       = None
    self._probationaryPeriod = probationaryPeriod
    self._numentaLearningPeriod  = numentaLearningPeriod


  def _computeLogLikelihood(self, likelihood):
    """
    Compute a log scale representation of the likelihood value. Since the
    likelihood computations return low probabilities that often go into 4 9"s or
    5 9"s, a log value is more useful for visualization, thresholding, etc.
    """
    # The log formula is:
    # Math.log(1.0000000001 - likelihood) / Math.log(1.0 - 0.9999999999);
    return math.log(1.0000000001 - likelihood) / -23.02585084720009


  def likelihood(self, value, anomalyScore, dttm):
    """
    Given the current metric value, plus the current anomaly
    score, output the anomalyLikelihood for this record.
    """
    dataPoint = (dttm, value, anomalyScore)
    # We ignore the first probationaryPeriod data points
    if len(self._historicalScores) < self._probationaryPeriod:
      likelihood = 0.5
    else:
      # On a rolling basis we re-estimate the distribution every 100 iterations
      if self._distribution is None or (self._iteration % 100 == 0):
        _, _, self._distribution = (
          anomaly_likelihood.estimateAnomalyLikelihoods(
            self._historicalScores,
            skipRecords = self._numentaLearningPeriod)
          )

      likelihoods, _, self._distribution = (
        anomaly_likelihood.updateAnomalyLikelihoods([dataPoint],
          self._distribution)
      )
      likelihood = 1.0 - likelihoods[0]

    # Before we exit update historical scores and iteration
    self._historicalScores.append(dataPoint)
    self._iteration += 1

    return likelihood